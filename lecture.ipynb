{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biol 359A  | Regularization and Sparse Regression\n",
    "## Lecture\n",
    "\n",
    "### Spring 2022, Week 6\n",
    "<hr>\n",
    "\n",
    "Objectives:\n",
    "-  Regularize complex linear models\n",
    "-  Evaluate performance of regularized models based on Cross-Validation\n",
    "-  Discuss the benefits and drawbacks of these methods\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import sklearn as sk\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import ipywidgets as widgets\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)}) #change figure size\n",
    "from sklearn import linear_model\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "TITLE_FONT = 20\n",
    "LABEL_FONT = 16\n",
    "TICK_FONT = 16\n",
    "FIG_SIZE = (10,10)\n",
    "COLORS= [\"#008080\",\"#CA562C\"]\n",
    "\n",
    "sns.set(font_scale=1.5, rc={'figure.figsize':FIG_SIZE}) \n",
    "sns.set_style(\"whitegrid\",  {'axes.linewidth': 2, 'axes.edgecolor':'black'})\n",
    "plt.rc(\"axes.spines\", top=False, right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function generates random *in silico* data according to a defined response. Since this is an *in silico* exercise, we know the underlying relationship. The data derives from a 2nd order system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic(x):\n",
    "    # Default - 2x^2 + 2\n",
    "    return 2*x**2 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_data(function, noise_std, n=10, measurement_std=.2, initial_value=0, x_max=3):\n",
    "    \"\"\"\n",
    "    This function generates noisy data with a certain amount of error applied to the function response.\n",
    "    The error is normally distributed around the noise_std.\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, x_max, n) \n",
    "    x_noise = np.random.normal(0, measurement_std, len(x))\n",
    "    x += x_noise\n",
    "    y_noise = np.random.normal(0, noise_std, len(x))\n",
    "    y = function(x) + initial_value\n",
    "    y += y_noise\n",
    "    plt.plot(x, y, 'C0.', label='data')\n",
    "    x_func = np.linspace(0, max(x)+measurement_std)\n",
    "    y_func = function(x_func) + initial_value\n",
    "    plt.plot(x_func, y_func, 'C0--', label='function')\n",
    "    plt.fill_between(x_func, y_func+noise_std, y_func-noise_std,\n",
    "                     alpha=0.1)          # Transparency of the fill\n",
    "    plt.title(r'$ y = 2x^2 + 2$ with noise (std of {})'.format(noise_std), fontsize=TITLE_FONT)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.xlim(0, max(x)+measurement_std)\n",
    "    plt.show()\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_model(x, y, x_model, y_model, title = ''):\n",
    "    \"\"\"\n",
    "    Plotter function.\n",
    "    \"\"\"\n",
    "    plt.plot(x,y, 'o', label='data')\n",
    "    plt.plot(x_model, y_model, '--', label='model')\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.xlim(0, max(x))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First we need to standardize the data\n",
    "Procedural note: **standardization** of data (taking the z-score by mean-centering and scaling data by its standard deviation) is required before performing regularization. An example of standardization is shown below. This code walks through the mathematics \"under the hood\" of `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_plot(x):\n",
    "    sns.distplot(x, label='Data')\n",
    "    mean = np.mean(x)\n",
    "    std = np.std(x)\n",
    "    x_zscore = (x - mean)/(std)\n",
    "    zscored_mean = np.mean(x_zscore)\n",
    "    zscored_std = np.std(x_zscore)\n",
    "    sns.distplot(x_zscore, label='Standardized')\n",
    "    plt.xlabel('x values')\n",
    "    plt.ylabel('frequencies')\n",
    "    plt.axvline(mean, color='C0', linestyle='-')\n",
    "    plt.axvline(mean + std, color='C0', linestyle='--')\n",
    "    plt.axvline(mean - std, color='C0', linestyle='--')\n",
    "    plt.axvline(zscored_mean, color='C1', linestyle='-')\n",
    "    plt.axvline(zscored_mean+zscored_std, color='C1', linestyle='--')\n",
    "    plt.axvline(zscored_mean-zscored_std, color='C1', linestyle='--')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "x = np.random.normal(6, 3, 300)\n",
    "zscore_plot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_feature_example(x, y, regularization = None, reg_lambda=1, degrees=6):\n",
    "    \"\"\"\n",
    "    Perform regularization on a polynomial feature set. \n",
    "    \"\"\"\n",
    "    poly_transform = PolynomialFeatures(degree=degrees, include_bias = False)\n",
    "    x_poly = poly_transform.fit_transform(x.reshape(-1,1))\n",
    "    \n",
    "    #Regularization techniques need to be scaled in order to work properly\n",
    "    x_scaler = StandardScaler().fit(x_poly)\n",
    "    y_scaler = StandardScaler().fit(y.reshape(-1,1))\n",
    "    x_poly_z = x_scaler.transform(x_poly)\n",
    "    y_z = y_scaler.transform(y.reshape(-1,1))\n",
    "    \n",
    "    #Code to perform the model fitting and parameter estimation\n",
    "    if regularization is None:\n",
    "        #Least Squares problem\n",
    "        plt.suptitle('Linear Regression', fontsize=20, fontweight='bold')\n",
    "        lm_poly = linear_model.LinearRegression(fit_intercept=True)\n",
    "        lm_poly.fit(x_poly_z,y_z)\n",
    "        \n",
    "    elif regularization is 'L1':\n",
    "        #LASSO problem\n",
    "        plt.suptitle('LASSO', fontsize=20, fontweight='bold')       \n",
    "        lm_poly = linear_model.Lasso(alpha = reg_lambda, max_iter=1e8, fit_intercept=True)\n",
    "        lm_poly.fit(x_poly_z,y_z)    \n",
    "        \n",
    "    elif regularization is 'L2':\n",
    "        #ridge problem\n",
    "        plt.suptitle('Ridge', fontsize=20, fontweight='bold')\n",
    "        lm_poly = linear_model.Ridge(alpha = reg_lambda, max_iter=1e5, fit_intercept=True)\n",
    "        lm_poly.fit(x_poly_z,y_z)\n",
    "        \n",
    "    x_model = np.linspace(min(x), max(x), 150).reshape(-1,1)\n",
    "    x_model_transform = poly_transform.fit_transform(x_model)\n",
    "    x_model_transform_z = x_scaler.transform(x_model_transform)\n",
    "    \n",
    "    \n",
    "    y_model = lm_poly.predict(x_model_transform_z)*y_scaler.scale_ + y_scaler.mean_\n",
    "    \n",
    "    #********************************************************************************\n",
    "    # Coefficients from scaled model can be transformed back into original units\n",
    "    # This code is outside the scope of this class and can be ignored. \n",
    "    \n",
    "    unscaled_coefficients = (lm_poly.coef_ * y_scaler.scale_ / x_scaler.scale_).flatten()\n",
    "    \n",
    "    poly_terms = [r'$({0:.3f})x ^ {{{1}}}$'.format(coef, i+1) for i, coef in enumerate(unscaled_coefficients)\n",
    "                 if coef != 0]\n",
    "    \n",
    "    unscaled_intercept = lm_poly.intercept_*y_scaler.scale_ + y_scaler.mean_ \\\n",
    "                            - sum(unscaled_coefficients*x_scaler.mean_)\n",
    "        \n",
    "    intercept_str = r'${0:.1f} + $'.format(unscaled_intercept[0])\n",
    "    title =  intercept_str + r'$+$'.join(poly_terms)\n",
    "    #********************************************************************************\n",
    "    \n",
    "    plot_model(x_data, y_data, x_model, y_model, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Plot the *in silico* data and the true model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = generate_noisy_data(quadratic, 1, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add polynomial degrees, and run an MLR.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_feature_example(x_data, y_data, degrees=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate the LASSO regression coefficients for the *in silico* data generated above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_feature_example(x_data, y_data, regularization='L1', reg_lambda=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculate the Ridge regression coefficients for the *in silico* data generated above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_feature_example(x_data, y_data, regularization='L2', reg_lambda = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Significantly increase the number of observations, *n*, and re-calculate the regression coefficients. \n",
    "When the number of samples, *n*, is high, we do not impose the risk of overfitting as it is unlikely we would propose a model with *n*-degrees of freedom when *n>>0*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_data, y_data = generate_noisy_data(quadratic, 1, n=300)\n",
    "polynomial_feature_example(x_data, y_data)\n",
    "polynomial_feature_example(x_data, y_data, regularization='L1', reg_lambda = .1)\n",
    "polynomial_feature_example(x_data, y_data, regularization='L2', reg_lambda = .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we determine an appropriate $\\lambda$?\n",
    "\n",
    "Here is where cross validation comes in. Let's do it with elastic net. Remember that we have a different cost function, with two $\\lambda$ parameters. $\\lambda_R$ describes the regularization penalty on the $L^2$ norm, and $\\lambda_L$ describes the penalty on $L^1$ norm.\n",
    "\n",
    "In order to work with the Elastic Net software in python, we are going to need to define these in terms of two other parameters. \n",
    "\n",
    "$$ \\lambda_L =  \\alpha * (L_1\\text{ratio}) $$\n",
    "$$ \\lambda_R =  \\alpha * (1 - L_1\\text{ratio}) $$\n",
    "\n",
    "Where $$ L_1\\text{ratio} = \\frac{\\lambda_L}{\\lambda_L + \\lambda_R}$$ \n",
    "\n",
    "We are going to go with a 80%/20% train/test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cross_validation(model, X, y, k=5):\n",
    "    scores = cross_validate(model, X, y, cv=k,\n",
    "                            scoring=('r2', 'neg_root_mean_squared_error'),\n",
    "                            return_train_score=True)\n",
    "    \n",
    "    temp_model = model.fit(X, y)\n",
    "    non_zero = np.count_nonzero(temp_model.coef_)\n",
    "    return np.mean(scores['train_r2']),np.mean(scores['test_r2']), non_zero\n",
    "\n",
    "def model_selection(X,y,test=False):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    y_scaler = StandardScaler().fit(y_train.reshape(-1,1))\n",
    "    X_z = X_scaler.transform(X_train)\n",
    "    y_z = y_scaler.transform(y_train.reshape(-1,1))\n",
    "    \n",
    "    \n",
    "    X_test_scaler = StandardScaler().fit(X_test)\n",
    "    y_test_scaler = StandardScaler().fit(y_test.reshape(-1,1))\n",
    "    X_test_z = X_scaler.transform(X_test)\n",
    "    y_test_z = y_scaler.transform(y_test.reshape(-1,1))\n",
    "    \n",
    "    alpha_dict = {}\n",
    "    p_dict = {}\n",
    "    i = 0\n",
    "    lim = 64\n",
    "    for alpha in [0, 1e-5, 1e-3, 1e-2, 1e-1, 0.5, 1, 5]:\n",
    "        alpha_dict[str(alpha)] = {}\n",
    "        p_dict[str(alpha)] = {}\n",
    "        for ratio in [0, .1, .5, .7, .9, .95, .99, 1]:\n",
    "            i += 1\n",
    "            print(f\"{i} of {lim}\")\n",
    "            model = linear_model.ElasticNet(alpha=alpha, l1_ratio=ratio) \n",
    "\n",
    "            train_r2, test_r2, non_zero = cross_validation(model, X_z, y_z)\n",
    "            alpha_dict[str(alpha)][str(ratio)] = f\"{train_r2:.4f}/{test_r2:.4f}\"\n",
    "            p_dict[str(alpha)][str(ratio)] = non_zero\n",
    "            \n",
    "    cv_df = pd.DataFrame(alpha_dict)\n",
    "    parameter_df = pd.DataFrame(p_dict)\n",
    "    cv_df.index.name = \"L1 Ratio\"\n",
    "    cv_df.style.set_caption(\"Alpha\")\n",
    "    parameter_df.index.name = \"L1 Ratio\"\n",
    "    parameter_df.style.set_caption(\"Alpha\")    \n",
    "    return cv_df, parameter_df, X_z, X_test_z, y_z, y_test_z\n",
    "\n",
    "\n",
    "x_reg, y_reg = make_regression(n_samples=100, n_features=100, \n",
    "                               n_informative=5, \n",
    "                               n_targets=1, bias=0.2,  \n",
    "                               noise=6, random_state=504)\n",
    "\n",
    "print(\"Columns = alpha\")\n",
    "print(\"Rows = L1_ratio\")\n",
    "cv_df, parameter_df, X_train, X_test, y_train, y_test = model_selection(x_reg, y_reg)\n",
    "print(\"R^2 from TRAIN/VALIDATION:\")\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of parameters in each model:\")\n",
    "parameter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parity_plot(true, pred, r_squared=None, title='', alpha=None, color=None, hue=None):\n",
    "    \"\"\"\n",
    "    plot true vs the predicted data\n",
    "    inputs: 2 list-like (arrays) data structures\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1,figsize=(10, 8))\n",
    "    if hue is not None:\n",
    "        sns.scatterplot(x=true, y=pred, hue=hue)\n",
    "    else: \n",
    "        if color is None: sns.scatterplot(x=true, y=pred)\n",
    "        else: sns.scatterplot(x=true, y=pred, alpha=alpha, color=color)\n",
    "    min_value = min(min(true), min(pred))\n",
    "    max_value = max(max(true), max(pred))\n",
    "    plt.plot([min_value, max_value],[min_value, max_value], '--', label=\"parity\")\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    sns.despine()\n",
    "    plt.text(1.01, 0.98, r\"$R^2 = {0:.2f}$\".format(r_squared),\n",
    "         ha='left', va='top', size =LABEL_FONT,\n",
    "         transform=ax.transAxes)\n",
    "    plt.title('Parity Plot: {}'.format(title), size=TITLE_FONT)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()    \n",
    "\n",
    "def test_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    parity_plot(y_train.flatten(), y_train_pred.flatten(), r_squared =model.score(X_train, y_train), title=\"Training Data\", color=\"grey\", alpha=0.5)\n",
    "    parity_plot(y_test.flatten(), y_test_pred.flatten(), r_squared =model.score(X_test, y_test), title=\"Test Data\", color=\"blue\", alpha=1)\n",
    "\n",
    "@widgets.interact_manual(alpha=[0, 1e-5, 1e-3, 1e-2, 1e-1, 0.5, 1, 5], l1_ratio=[0, .1, .5, .7, .9, .95, .99, 1])\n",
    "def build_and_test(alpha, l1_ratio):\n",
    "    model = linear_model.ElasticNet(alpha=alpha, l1_ratio=l1_ratio) \n",
    "    test_model(model, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
